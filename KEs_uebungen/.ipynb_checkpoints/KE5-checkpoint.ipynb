{
 "cells": [
  {
   "cell_type": "raw",
   "id": "15b94051",
   "metadata": {},
   "source": [
    "Name:\n",
    "Matrikelnummer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b8e40",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5670741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:38.849452Z",
     "start_time": "2022-05-20T13:30:38.731148Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "#from pyspark.sql import SparkSession\n",
    "#import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da065",
   "metadata": {},
   "source": [
    "# (I can't get no) satisfaction\n",
    "Wir arbeiten weiterhin mit dem Datensatz über die Zufriedenheit von Angestellten einer fiktiven Firma. In dieser Kurseinheit betrachten wir die parallele Verarbeitung mit Spark. \n",
    "\n",
    "*Anmerkung: Der verwendete Datensatz ist natürlich eher klein, so dass normalerweise keine Parallelisierung benötigt wird. In der Kurseinheit dient er aber dazu, die grundlegenden Konzepte von Spark zu vermitteln.*\n",
    "\n",
    "*Wichtige Methoden finden Sie im Foliensatz, es empfiehlt sich jedoch auch, die <a href=\"https://spark.apache.org/docs/latest/api/python/reference/index.html\">pyspark-Doku</a> für die Bearbeitung zu nutzen.*\n",
    "\n",
    "*In dieser Kurseinheit werden u.a. Lambda-Funktionen verwendet. Wenn Sie damit noch nicht vetraut sind, hilft ein Blick in die <a href=\"https://www.w3schools.com/python/python_lambda.asp\">Python-Lambas-Doku</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb036c",
   "metadata": {},
   "source": [
    "Den Eintrittspunkt in die Funktionalität von Spark stellt der SparkContext dar. Dieser repräsentiert auch die Verbindung zu den Exekutoren. Bei der Erstellung wird außerdem eine Sammlung von Konfigurationsparametern mitgegeben. Wir beschränken uns zunächst auf die Angabe des Masters und einem App-Namen:\n",
    "- **master:** Wir verwenden YARN als Ressourcen-Manager und starten die Anwendung im Client-Modus.\n",
    "- **appName:** Hier vergeben wir einen Namen für unsere Applikation.\n",
    "\n",
    "Da wir später auch mit SparkSQL arbeiten wollen, erstellen wir zudem direkt eine SparkSession. Diese stellt den Eingangspunkt für die SparkSQL-Funktionalität zur Verfügung und beinhaltet auch den SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db27dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:52.527516Z",
     "start_time": "2022-05-20T13:30:39.959013Z"
    }
   },
   "outputs": [],
   "source": [
    "# SparkSession erstellen\n",
    "spark = \n",
    "\n",
    "# SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87597377",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "Zunächst betrachten wir die Arbeit mit RDDs. Wie Sie in den Vorlesungsvideos gelernt haben, bilden diese die Grundlage der parallelen Verarbeitung. In der Praxis wird meist nicht direkt mit RDDs, sondern mit einer High-level-Bibliothek wie SparkSQL gearbeitet. Es ist jedoch wichtig, die Konzepte von RDDs zu verstehen, da diese die Basis von Spark darstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0df681",
   "metadata": {},
   "source": [
    "### Eigenschaften von RDDs\n",
    "Betrachten wir zunächst die wichtigsten Eigenschaften von RDDs:\n",
    "- In-Memory-Verarbeitung\n",
    "- Lazy\n",
    "- Partitioniert\n",
    "- Unveränderlich\n",
    "- Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48079e69",
   "metadata": {},
   "source": [
    "#### In-Memory-Verarbeitung\n",
    "Beschreiben Sie die Vorteile der In-Memory-Verarbeitung."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ab0c320",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7ab8d",
   "metadata": {},
   "source": [
    "#### Lazy\n",
    "Beschreiben Sie, was es für die Programmierung bedeutet, dass die Verarbeitung lazy ausgeführt wird. Programmieren Sie ein Code-Beispiel Ihrer Wahl, in dem Sie die Auswirkungen der lazy Ausführung demonstrieren. Nennen Sie zudem die wichtigsten Unterschiede zwischen Transformations und Actions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "813acaa4",
   "metadata": {},
   "source": [
    "Auswirkungen der lazy Ausführung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ca89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:30.836829Z",
     "start_time": "2022-05-18T12:14:30.600665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihr Beispiel für eine lazy Ausführung"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b41bc1",
   "metadata": {},
   "source": [
    "Wichtigste Unterschiede zwischen Transformations und Actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00336b",
   "metadata": {},
   "source": [
    "#### Partitioniert\n",
    "Erstellen Sie aus der vorgegebenen Liste (`data`) ein RDD namens `rdd_part` mit drei Partitionen. Kontrollieren Sie nach der Erstellung, dass die korrekte Anzahl Partitionen erzeugt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe841dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:57.413324Z",
     "start_time": "2022-05-18T12:14:57.409659Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [1 for i in range (10)] + [0 for i in range (5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146eee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:05:34.579627Z",
     "start_time": "2022-05-16T12:05:34.569123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca697ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:19.658042Z",
     "start_time": "2022-05-18T12:15:19.651377Z"
    }
   },
   "outputs": [],
   "source": [
    "# RDD erstellen\n",
    "rdd_part ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f287ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:23.007815Z",
     "start_time": "2022-05-18T12:15:22.996956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Anzahl Partitionen anzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821b4d0",
   "metadata": {},
   "source": [
    "Im Folgenden schauen wir uns die Verteilung der Daten auf die unterschiedlichen Partitionen an.\n",
    "\n",
    "> **Achtung:** Die Anwendung von `collect()` in Kombination mit `glom()` ist nur für kleine Datensätze geeignet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b296885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:31.503029Z",
     "start_time": "2022-05-18T12:15:31.234810Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd_part.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c06286",
   "metadata": {},
   "source": [
    "Wie wir sehen können, bestehen zwei Partitionen aus 1en und eine aus 0en. Filtern Sie das RDD nun so, dass nur die Werte größer 0 übrig bleiben. Erstellen Sie hierfür ein neues RDD namens `rdd_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cbab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:38.268146Z",
     "start_time": "2022-05-18T12:15:38.263951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "rdd_filtered ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b3993",
   "metadata": {},
   "source": [
    "Wenn wir uns nun die Verteilung der Daten auf die Partitionen ansehen, sehen wir, dass wir eine leere Partition erzeugt haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c007e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:42.674871Z",
     "start_time": "2022-05-18T12:15:42.499927Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd_filtered.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22c444",
   "metadata": {},
   "source": [
    "Beschreiben Sie die Nachteile von leeren Partitionen. Wie kann dieses Problem gelöst werden (Stichwort *Shuffling*) und warum sollte Shuffling normalerweise vermieden werden?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae0d5997",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85b69c",
   "metadata": {},
   "source": [
    "#### Unveränderlich und Lineage\n",
    "Warum sind RDDs unveränderlich und warum kennt jedes RDD seine Abstammung (Lineage)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dfb6147",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbe486",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie den Datensatz `employees_satisfaction_transformed.csv` in ein RDD. Denken Sie daran, dass die Daten standardmäßig aus dem HDFS geladen werden. Aus diesem Grund müssen die Daten zunächst ins HDFS geladen werden. Verwenden Sie hierzu das Notebook `hdfs-upload.ipynb`, das wir Ihnen in Moodle zur Verfügung gestellt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b5ee",
   "metadata": {},
   "source": [
    "Laden Sie anschließend die Daten in ein RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:45.740759Z",
     "start_time": "2022-05-18T12:16:45.691953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a817",
   "metadata": {},
   "source": [
    "### Erste Datenexploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281400",
   "metadata": {},
   "source": [
    "Lassen Sie sich dann die ersten zehn Zeilen des RDDs anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df4de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:49.061163Z",
     "start_time": "2022-05-18T12:16:48.403315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d99e5f",
   "metadata": {},
   "source": [
    "Die erste Zeile entspricht dem Header. Entfernen Sie diese aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcb216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:55.070359Z",
     "start_time": "2022-05-18T12:16:54.946567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f30438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:23:31.207384Z",
     "start_time": "2022-05-16T12:23:31.090921Z"
    }
   },
   "source": [
    "Zählen Sie nun die Anzahl der Angestellten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ff2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:17:03.912327Z",
     "start_time": "2022-05-18T12:17:03.303209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c66d9c",
   "metadata": {},
   "source": [
    "### Datenverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc40b9",
   "metadata": {},
   "source": [
    "Zählen Sie nun die Angestellten pro Department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499b286",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Sollten Sie keinen Lösungsansatz haben, finden Sie unter dem Button \"Show Solution\" die Beschreibung der notwendigen Verarbeitungs-Schritte.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f383a4",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. Aktuell ist jede Zeile des RDD ein String, in dem die einzelnen Felder durch Komma getrennt sind. Dieser String muss zunächst gesplittet werden.\n",
    "2. Im nächsten Schritt müssen Key-Value-Paare gebildet werden. Der Key ist das Department und der Wert ist 1.\n",
    "3. Dann müssen pro Key alle Werte addiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71226012",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Wenn Sie Probleme mit der konkreten Umsetzung der Aufgabe haben, finden Sie unter dem Button \"Show Solution\" Hinweise zur Programmierung.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd24fd",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Das Splitten der einzelnen Funktionen erfolgt mit `split(\",\")`.\n",
    "- Anschließend liegen die Daten in einer Liste vor. Das dritte Element (`line[2]`) entspricht dem Department.\n",
    "- Mithilfe der Funktion `reducyByKey()` können die Werte pro Key addiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b975a2e",
   "metadata": {},
   "source": [
    "Die Arbeit mit RDDs ist offensichtlich recht mühsam, weshalb in der Praxis häufig eine High-level API wie SparkSQL verwendet wird. Diese schauen wir uns im Folgenden an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926af8d",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "Im Folgenden arbeiten wir mit der High-level API SparkSQL.\n",
    "\n",
    "*Vorsicht: DataFrames sind die zentrale Abstraktion der SparkSQL-Bibiliothek. Die Bibliothek bietet zwei praktisch gleichwertige APIs, um mit DataFrames zu arbeiten: Die **DataFrame API** und die **SQL API**. Wir arbeiten zunächst mit der DataFrame API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b8b35",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie die Daten aus der Datei `employees_satisfaction_transformed.csv` in ein DataFrame. Die Spaltenbezeichnungen sollen dabei im Header stehen und nicht Teil der Daten sein, nutzen Sie außerdem die automatische Schemaerkennung von Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f49c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:05.041562Z",
     "start_time": "2022-05-20T13:30:58.623713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af8746",
   "metadata": {},
   "source": [
    "Lassen Sie sich das Schema ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1cc9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:05.871051Z",
     "start_time": "2022-05-20T13:31:05.855291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82feb5",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Daten anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf2891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:07.726961Z",
     "start_time": "2022-05-20T13:31:07.392422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab027004",
   "metadata": {},
   "source": [
    "Geben Sie die Anzahl Angestellter aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c639af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:09.506780Z",
     "start_time": "2022-05-20T13:31:08.979554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc66789",
   "metadata": {},
   "source": [
    "DataFrames basieren auf RDDs. Deshalb kann auch jedes DataFrame in ein RDD umgewandelt werden.\n",
    "\n",
    "Erstellen Sie aus dem DataFrame der Angestellten ein RDD und lassen Sie sich die ersten fünf Zeilen anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74083f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:10.911287Z",
     "start_time": "2022-05-20T13:31:10.267345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b998b3e",
   "metadata": {},
   "source": [
    "Das RDD besteht aus einzelnen Rows. Mittels Index kann auf die einzelnen Rows zugegriffen werden. Der Zugriff auf die Spalten erfolgt entweder auch durch die Angabe des Index oder durch den Spaltennamen.\n",
    "\n",
    "Lassen Sie sich das Gehalt der ersten Row ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0a8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:12.144784Z",
     "start_time": "2022-05-20T13:31:11.878447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5c185",
   "metadata": {},
   "source": [
    "### Verarbeitung\n",
    "*Hinweis: Für die Bearbeitung der folgenden Aufaben ist es hilfreich, sich die `pyspark.sql.functions` anzusehen. Diese sind bereits importiert und können mit `f.<function_name>` verwendet werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0134cb",
   "metadata": {},
   "source": [
    "Fügen Sie eine neue Spalte `service_days` hinzu, in der angegeben wird, seit wie vielen Tagen ein\\*e Angestellte\\*r im Unternehmen ist. Verwenden Sie als Stichtag den 01.01.2021 (`comparison`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:13.934683Z",
     "start_time": "2022-05-20T13:31:13.931821Z"
    }
   },
   "outputs": [],
   "source": [
    "comparison = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b6240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:14.474401Z",
     "start_time": "2022-05-20T13:31:14.416892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc920c",
   "metadata": {},
   "source": [
    "Lassen Sie sich anschließend das Eintrittsdatum und die Tage im Unternehmen von allen Angestellten anzeigen, die länger als 6000 Tage im Unternehmen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2203dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:16.272880Z",
     "start_time": "2022-05-20T13:31:16.006660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0959",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971c46d",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Departments des Datensatzes anzeigen (jedes nur einmal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:23.044052Z",
     "start_time": "2022-05-20T13:31:22.674850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9ad56",
   "metadata": {},
   "source": [
    "Ermitteln Sie (wie eben bei der Arbeit mit RDDs) die Anzahl Angestellte pro Department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869911f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:24.046796Z",
     "start_time": "2022-05-20T13:31:23.715420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c8e3f",
   "metadata": {},
   "source": [
    "Geben Sie durchschnittliche Zufriedenheit aller Personen aus, die in der Abteilung *HR* arbeiten, sowie das Alter der ältesten Person der Abteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:26.924816Z",
     "start_time": "2022-05-20T13:31:26.677928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f5e3f",
   "metadata": {},
   "source": [
    "Wie erstellen nun ein weiteres DataFrame. Erstellen Sie aus den Daten in `dept_data` das DataFrame `departments` mit den angegebenen Spaltennamen. Geben Sie das erzeugte DataFrame aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884ec5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:30.025682Z",
     "start_time": "2022-05-20T13:31:30.018246Z"
    }
   },
   "outputs": [],
   "source": [
    "dept_data = [(\"Sales\", \"New York\", 10001), \n",
    "            (\"HR\", \"Los Angeles\", 90001),\n",
    "            (\"Purchasing\", \"San Francisco\", 94104),\n",
    "            (\"Marketing\", \"Philadelphia\", 19110),\n",
    "            (\"Technology\", \"Los Angeles\", 90099)]\n",
    "\n",
    "column_names = [\"name\", \"location\", \"zip_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:31.582707Z",
     "start_time": "2022-05-20T13:31:30.647816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbddabb",
   "metadata": {},
   "source": [
    "Wie viele Angestellte arbeiten in Los Angeles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91683a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:32.932839Z",
     "start_time": "2022-05-20T13:31:32.346598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794ada0",
   "metadata": {},
   "source": [
    "Lassen Sie sich die ersten 10 Gehälter der Angestellten ausgeben, die an der Ostküste arbeiten (in diesem Fall `zip_code < 90000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bffef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:35.699842Z",
     "start_time": "2022-05-20T13:31:35.467405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11623a09",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Durch die In-Memory-Verarbeitung werden DataFrames in der Regel nicht zwischengespeichert. Mit jeder Action werden alle Verarbeitungsschritte neu ausgeführt. Manchmal kann es aber sinnvoll sein, ein DataFrame zwischenzuspeichern (zu *cachen*). Beim Aufruf der ersten Action wird das DataFrame dann im Speicher gehalten.\n",
    "\n",
    "(*Hinweis*: Gleiches gilt natürlich auch für RDDs, da DataFrames ja auf RDDs basieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "Betrachten Sie den folgenden Code: An welcher Stelle könnte es sinnvoll sein, ein Caching durchzuführen und warum? Fügen Sie den Code für das Caching ein. Schreiben Sie in die untere Zelle Ihre Begründung und beschreiben Sie, welche DataFrames auf die gecachte Version zugreifen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e30d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "```python\n",
    "df1 = spark.read.csv(path = \"my_data.csv\", header = True)\n",
    "df2 = df1.filter(df1.salary.between(40000, 50000))\n",
    "df3 = df2.select(\"name\", \"salary\", \"department\", \"satisfied\")\n",
    "df4 = df3.groupby(\"department\").agg({\"satisfied\": \"mean\"})\n",
    "df4.show()\n",
    "df5 = df3.groupby(\"department\").agg({\"salary\": \"mean\"})\n",
    "df5.show()\n",
    "df6 = df3.select(\"name\").where(df3[\"satisfied\"] == 1)\n",
    "df6.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "878fb55f",
   "metadata": {},
   "source": [
    "Ihre Begründung:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558d965",
   "metadata": {},
   "source": [
    "## SQL\n",
    "Im Folgenden verwenden wir SQL zur Datenabfrage. Dafür erstellen wir zunächst eine temporale View, auf der dann die SQL-Abfragen erfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d524019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:52.806431Z",
     "start_time": "2022-05-18T12:26:51.663985Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\\\n",
    "CREATE TEMP VIEW employees \\\n",
    "USING CSV \\\n",
    "OPTIONS (path = 'employees_satisfaction_transformed.csv', header = 'True', inferSchema = 'True')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3219ad",
   "metadata": {},
   "source": [
    "Ermitteln Sie nun wieder die Anzahl der Angestellten pro Department, aber verwenden Sie dieses Mal SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30abf3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:54.656380Z",
     "start_time": "2022-05-18T12:26:54.394996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb4653",
   "metadata": {},
   "source": [
    "## Ressourcen-Vebrauch\n",
    "\n",
    "Sie arbeiten in diesen Übungsaufgaben mit ihrem eigenen Cluster. In der Praxis ist dies natürlich selten der Fall, hier werden meist viele Jobs gleichzeitig gestartet und konkurrieren um Ressourcen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ba7dd",
   "metadata": {},
   "source": [
    "Öffnen Sie die __[WebUI von YARN](http://127.0.0.1:8088)__ und sehen sich die aktuellen Applikationen an. Sie sollten dort das aktuelle Jupyter Notebook finden. Solange die SparkSession (der SparkContext) erstellt ist, wird das Notebook dort als *RUNNING* aufgelistet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbe4f3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Wie viele Kerne und wie viel Speicher verbraucht die Spark-Applikation? Wie können Sie diese Summe berechnen?\n",
    "\n",
    "*Hinweis: Werfen Sie einen Blick in die Datei `spark-defaults.conf` im `SPARK_CONF_DIR`.*\n",
    "\n",
    "*Sollten Sie nicht wissen, wie Sie diese Datei finden können, finden Sie unter dem \"Show Solution\"-Button hilfreiche Kommandos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a9af2",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Über \"New\" -> \"Terminal\" können Sie ein Terminal öffnen. Alternativ können Sie in einer Code-Zelle einen Bash-Befehl ausführen, indem Sie ein ! an den Anfang der Zelle schreiben\n",
    "- Zugriff auf den Inhalt einer Umgebungs-Variablen mittels `$`, z.B. `$SPARK_CONF_DIR`\n",
    "- Anzeigen von Dateiinhalten: `cat`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87b95f3",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953cbef",
   "metadata": {},
   "source": [
    "Stoppen Sie den SparkContext, um alle Ressourcen, die dieses Notebook benötigt, freizugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50743887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-17T07:34:36.480924Z",
     "start_time": "2022-08-17T07:34:36.474478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba990b91",
   "metadata": {},
   "source": [
    "Öffnen Sie ein Terminal und starten Sie eine pyspark-Shell mit YARN als Ressourcen-Manager im client-mode mit 2 Exekutoren mit jeweils 1GB Memory und 2 Cores. Wie viele Cores und wie viel Speicher benötigt die Applikation und wie setzen sich diese Zahlen zusammen? Kontrollen Sie in der WebUI von YARN, ob Ihre Berechnungen korrekt sind."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a8b432",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980a463",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal eine weitere pyspark-Shell (ohne die erste Shell zu beenden) mit YARN als Ressourcen-Manager im client-mode mit der gleichen Konfiguration (2 Exekutoren mit jeweils 1GB Memory und 2 Cores). Erstellen Sie dort ein RDD mit den Werten 1, 2, 3, 4 und lassen sich dieses mittels `collect()` ausgeben.\n",
    "\n",
    "Was passiert und warum? Wie können Sie das Problem beheben?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc390ab8",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b01a8",
   "metadata": {},
   "source": [
    "Schließen Sie beide Shells und starten anschließend eine neue pyspark-Shell mit folgenden Eigenschaften:\n",
    "- YARN als Ressourcen-Manager im client-mode\n",
    "- 2 Exekutoren mit jeweils\n",
    "    - 2GB Memory\n",
    "    - 3 Cores\n",
    "    \n",
    "Wie viele Ressourcen vebraucht diese Applikation und wie setzen sich diese zusammen?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d7b3eac",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fc5a1",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal noch eine pypsark-Shell (ohne die erste Shell zu beenden) mit folgendem Befehl:\n",
    "\n",
    "`pyspark --master yarn --conf spark.yarn.am.cores=3`\n",
    "\n",
    "Was passiert und warum? Überprüfen Sie auch den Status der Applikation in der WebUI von YARN."
   ]
  },
  {
   "cell_type": "raw",
   "id": "abaeaf3c",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
