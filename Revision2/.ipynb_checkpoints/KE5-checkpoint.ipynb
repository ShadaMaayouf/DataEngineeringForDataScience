{
 "cells": [
  {
   "cell_type": "raw",
   "id": "15b94051",
   "metadata": {},
   "source": [
    "Name:\n",
    "Matrikelnummer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b8e40",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5670741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:38.849452Z",
     "start_time": "2022-05-20T13:30:38.731148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4459c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InsecureClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mInsecureClient\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:50070\u001b[39m\u001b[38;5;124m'\u001b[39m, user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_username\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InsecureClient' is not defined"
     ]
    }
   ],
   "source": [
    "client = InsecureClient('http://localhost:50070', user='your_username')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da065",
   "metadata": {},
   "source": [
    "# (I can't get no) satisfaction\n",
    "Wir arbeiten weiterhin mit dem Datensatz über die Zufriedenheit von Angestellten einer fiktiven Firma. In dieser Kurseinheit betrachten wir die parallele Verarbeitung mit Spark. \n",
    "\n",
    "*Anmerkung: Der verwendete Datensatz ist natürlich eher klein, so dass normalerweise keine Parallelisierung benötigt wird. In der Kurseinheit dient er aber dazu, die grundlegenden Konzepte von Spark zu vermitteln.*\n",
    "\n",
    "*Wichtige Methoden finden Sie im Foliensatz, es empfiehlt sich jedoch auch, die <a href=\"https://spark.apache.org/docs/latest/api/python/reference/index.html\">pyspark-Doku</a> für die Bearbeitung zu nutzen.*\n",
    "\n",
    "*In dieser Kurseinheit werden u.a. Lambda-Funktionen verwendet. Wenn Sie damit noch nicht vetraut sind, hilft ein Blick in die <a href=\"https://www.w3schools.com/python/python_lambda.asp\">Python-Lambas-Doku</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb036c",
   "metadata": {},
   "source": [
    "Den Eintrittspunkt in die Funktionalität von Spark stellt der SparkContext dar. Dieser repräsentiert auch die Verbindung zu den Exekutoren. Bei der Erstellung wird außerdem eine Sammlung von Konfigurationsparametern mitgegeben. Wir beschränken uns zunächst auf die Angabe des Masters und einem App-Namen:\n",
    "- **master:** Wir verwenden YARN als Ressourcen-Manager und starten die Anwendung im Client-Modus.\n",
    "- **appName:** Hier vergeben wir einen Namen für unsere Applikation.\n",
    "\n",
    "Da wir später auch mit SparkSQL arbeiten wollen, erstellen wir zudem direkt eine SparkSession. Diese stellt den Eingangspunkt für die SparkSQL-Funktionalität zur Verfügung und beinhaltet auch den SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db27dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:52.527516Z",
     "start_time": "2022-05-20T13:30:39.959013Z"
    }
   },
   "outputs": [],
   "source": [
    "# SparkSession erstellen\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87597377",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "Zunächst betrachten wir die Arbeit mit RDDs. Wie Sie in den Vorlesungsvideos gelernt haben, bilden diese die Grundlage der parallelen Verarbeitung. In der Praxis wird meist nicht direkt mit RDDs, sondern mit einer High-level-Bibliothek wie SparkSQL gearbeitet. Es ist jedoch wichtig, die Konzepte von RDDs zu verstehen, da diese die Basis von Spark darstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0df681",
   "metadata": {},
   "source": [
    "### Eigenschaften von RDDs\n",
    "Betrachten wir zunächst die wichtigsten Eigenschaften von RDDs:\n",
    "- In-Memory-Verarbeitung\n",
    "- Lazy\n",
    "- Partitioniert\n",
    "- Unveränderlich\n",
    "- Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48079e69",
   "metadata": {},
   "source": [
    "#### In-Memory-Verarbeitung\n",
    "Beschreiben Sie die Vorteile der In-Memory-Verarbeitung."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ab0c320",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7ab8d",
   "metadata": {},
   "source": [
    "#### Lazy\n",
    "Beschreiben Sie, was es für die Programmierung bedeutet, dass die Verarbeitung lazy ausgeführt wird. Programmieren Sie ein Code-Beispiel Ihrer Wahl, in dem Sie die Auswirkungen der lazy Ausführung demonstrieren. Nennen Sie zudem die wichtigsten Unterschiede zwischen Transformations und Actions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "813acaa4",
   "metadata": {},
   "source": [
    "Auswirkungen der lazy Ausführung:\n",
    "In der Programmierung bedeutet “Lazy-Ausführung” (oder “Lazy-Evaluation”), dass Berechnungen erst dann ausgeführt werden, wenn sie tatsächlich benötigt werden. Dies kann die Effizienz von Programmen verbessern, indem unnötige Berechnungen vermieden und Ressourcen gespart werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8ca89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:30.836829Z",
     "start_time": "2022-05-18T12:14:30.600665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ein gutes Beispiel für Lazy-Ausführung ist die Verwendung von Generatoren in Python. \n",
    "Ein Generator erzeugt Werte “on the fly”, aber nur wenn sie benötigt werden. \n",
    "'''\n",
    "def count_up_to(n):\n",
    "    count = 1\n",
    "    while count <= n:\n",
    "        yield count\n",
    "        count += 1\n",
    "\n",
    "# Erstellen des Generatord, aber es wird noch nichts berechnet\n",
    "numbers = count_up_to(5)\n",
    "\n",
    "# Die Zahlen werden erst generiert, wenn wir durch sie iterieren\n",
    "for number in numbers:\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b41bc1",
   "metadata": {},
   "source": [
    "Wichtigste Unterschiede zwischen Transformations und Actions:\n",
    "\n",
    "In Apache Spark sind Transformationen und Aktionen zwei Arten von Operationen, die auf Daten angewendet werden können. Hier sind die wichtigsten Unterschiede:\n",
    "\n",
    "- **Transformationen** erstellen ein neues Dataset aus einem vorhandenen Dataset. Beispiele für Transformationen sind `map`, `filter` und `reduceByKey`. Transformationen in Spark sind \"lazy\", was bedeutet, dass sie erst ausgeführt werden, wenn eine Aktion aufgerufen wird. Sie führen also keine Berechnungen durch, wenn sie definiert werden, sondern erstellen lediglich einen Ausführungsplan.\n",
    "\n",
    "- **Aktionen** hingegen liefern einen Wert an den Treiberprogramm zurück oder schreiben Daten in ein externes Speichersystem. Aktionen lösen die tatsächliche Berechnung aus, die durch die Transformationen definiert wurde. Beispiele für Aktionen sind `count`, `first`, `take`, `collect` und `save`.\n",
    "\n",
    "Zusammengefasst, Transformationen erstellen einen Ausführungsplan, aber berechnen nichts, bis eine Aktion aufgerufen wird. Aktionen lösen die Berechnung aus und geben das Ergebnis zurück. Dieses Konzept ermöglicht es Spark, seine Berechnungen zu optimieren und effizient über große Datenmengen hinweg durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00336b",
   "metadata": {},
   "source": [
    "#### Partitioniert\n",
    "Erstellen Sie aus der vorgegebenen Liste (`data`) ein RDD namens `rdd_part` mit drei Partitionen. Kontrollieren Sie nach der Erstellung, dass die korrekte Anzahl Partitionen erzeugt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe841dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:57.413324Z",
     "start_time": "2022-05-18T12:14:57.409659Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [1 for i in range (10)] + [0 for i in range (5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca697ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:19.658042Z",
     "start_time": "2022-05-18T12:15:19.651377Z"
    }
   },
   "outputs": [],
   "source": [
    "# RDD erstellen\n",
    "rdd_part =  sc.parallelize(data, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a146eee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:05:34.579627Z",
     "start_time": "2022-05-16T12:05:34.569123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "num_partitions = rdd_part.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f287ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:23.007815Z",
     "start_time": "2022-05-18T12:15:22.996956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das RDD hat 3 Partitionen.\n"
     ]
    }
   ],
   "source": [
    "# Anzahl Partitionen anzeigen\n",
    "'''\n",
    "n diesem Code wird ein RDD namens rdd_part erstellt, das die Daten aus der Liste data enthält und in drei Partitionen aufgeteilt ist. Die Anzahl der Partitionen wird dann mit getNumPartitions überprüft und ausgegeben.'''\n",
    "\n",
    "print(f\"Das RDD hat {num_partitions} Partitionen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821b4d0",
   "metadata": {},
   "source": [
    "Im Folgenden schauen wir uns die Verteilung der Daten auf die unterschiedlichen Partitionen an.\n",
    "\n",
    "> **Achtung:** Die Anwendung von `collect()` in Kombination mit `glom()` ist nur für kleine Datensätze geeignet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b296885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:31.503029Z",
     "start_time": "2022-05-18T12:15:31.234810Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (192.168.1.134 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd_part\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (192.168.1.134 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd_part.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c06286",
   "metadata": {},
   "source": [
    "Wie wir sehen können, bestehen zwei Partitionen aus 1en und eine aus 0en. Filtern Sie das RDD nun so, dass nur die Werte größer 0 übrig bleiben. Erstellen Sie hierfür ein neues RDD namens `rdd_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430cbab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:38.268146Z",
     "start_time": "2022-05-18T12:15:38.263951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "rdd_filtered = rdd_part.filter(lambda x: x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b3993",
   "metadata": {},
   "source": [
    "Wenn wir uns nun die Verteilung der Daten auf die Partitionen ansehen, sehen wir, dass wir eine leere Partition erzeugt haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96c007e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:42.674871Z",
     "start_time": "2022-05-18T12:15:42.499927Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3) (192.168.1.134 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\PycharmProjects\\PAndasTutorial\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3) (192.168.1.134 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1140)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1074)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:500)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1111)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd_filtered.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22c444",
   "metadata": {},
   "source": [
    "Beschreiben Sie die Nachteile von leeren Partitionen. Wie kann dieses Problem gelöst werden (Stichwort *Shuffling*) und warum sollte Shuffling normalerweise vermieden werden?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae0d5997",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "\n",
    "Leere Partitionen in einem RDD können zu mehreren Problemen führen:\n",
    "\n",
    "1. **Ressourcenverschwendung**: Jede Partition wird von einem separaten Task verarbeitet. Wenn eine Partition leer ist, wird ein Task verschwendet, da es nichts zu verarbeiten gibt.\n",
    "2. **Ungleichmäßige Lastverteilung**: Wenn einige Partitionen viele Daten enthalten und andere leer sind, kann dies zu einer ungleichmäßigen Lastverteilung führen. Einige Tasks können sehr lange dauern, während andere schnell abgeschlossen sind.\n",
    "\n",
    "Um das Problem leerer Partitionen zu lösen, kann man das RDD neu partitionieren, um die Daten gleichmäßiger zu verteilen. Dieser Prozess wird als **Shuffling** bezeichnet. Mit der `repartition()` Funktion in Spark kann man die Anzahl der Partitionen in einem RDD ändern:\n",
    "\n",
    "```python\n",
    "rdd_repartitioned = rdd_filtered.repartition(10)\n",
    "```\n",
    "\n",
    "Obwohl Shuffling das Problem leerer Partitionen lösen kann, sollte es normalerweise vermieden werden, da es mit hohen Kosten verbunden ist:\n",
    "\n",
    "1. **Hoher Netzwerkverkehr**: Beim Shuffling werden alle Daten über das Netzwerk gesendet, was zu hohem Netzwerkverkehr führen kann.\n",
    "2. **Hoher Disk I/O**: Die Daten müssen auf die Festplatte geschrieben und von dort gelesen werden, was zu hohem Disk I/O führt.\n",
    "3. **Hoher CPU-Verbrauch**: Das Sortieren und Aggregieren der Daten erfordert CPU-Ressourcen.\n",
    "\n",
    "Daher sollte Shuffling nur dann verwendet werden, wenn es unbedingt notwendig ist, z.B. wenn die Vorteile der Neuverteilung der Daten die Kosten des Shufflings überwiegen. Es ist immer eine gute Praxis, die Daten so früh wie möglich im Verarbeitungsprozess zu partitionieren, um die Notwendigkeit von Shuffling zu minimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85b69c",
   "metadata": {},
   "source": [
    "#### Unveränderlich und Lineage\n",
    "Warum sind RDDs unveränderlich und warum kennt jedes RDD seine Abstammung (Lineage)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dfb6147",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "Resilient Distributed Datasets (RDDs) sind unveränderlich, was bedeutet, dass sie nach ihrer Erstellung nicht mehr verändert werden können. Dies hat mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Da RDDs unveränderlich sind, können sie bei einem Fehler einfach neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Parallelisierung**: Unveränderlichkeit erleichtert die Parallelisierung von Operationen, da es keine Synchronisationsprobleme gibt, die normalerweise bei veränderlichen Daten auftreten.\n",
    "3. **Effizienz**: Da RDDs unveränderlich sind, können sie effizient auf mehreren Knoten verteilt und zwischengespeichert werden, was die Verarbeitungsgeschwindigkeit verbessert.\n",
    "\n",
    "Jedes RDD kennt seine Abstammung oder \"Lineage\", was bedeutet, dass es die Reihe von Transformationen kennt, die zu seiner Erstellung geführt haben. Dies hat auch mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Wenn ein RDD verloren geht, kann es anhand seiner Lineage neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Optimierung**: Die Kenntnis der Lineage ermöglicht es Spark, die Verarbeitung zu optimieren. Zum Beispiel kann Spark entscheiden, einige Operationen zusammenzufassen oder die Reihenfolge der Operationen zu ändern, um die Effizienz zu verbessern.\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass die Unveränderlichkeit und die Kenntnis der Lineage zwei der Schlüsseleigenschaften von RDDs sind, die Spark seine Robustheit und Effizienz verleihen. Sie ermöglichen eine effiziente verteilte Verarbeitung und eine robuste Fehlerbehandlung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbe486",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie den Datensatz `employees_satisfaction_transformed.csv` in ein RDD. Denken Sie daran, dass die Daten standardmäßig aus dem HDFS geladen werden. Aus diesem Grund müssen die Daten zunächst ins HDFS geladen werden. Verwenden Sie hierzu das Notebook `hdfs-upload.ipynb`, das wir Ihnen in Moodle zur Verfügung gestellt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b5ee",
   "metadata": {},
   "source": [
    "Laden Sie anschließend die Daten in ein RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c317b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:45.740759Z",
     "start_time": "2022-05-18T12:16:45.691953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "rdd_df = sc.textFile(\"hdfs://localhost:8888/path/to/employees_satisfaction_transformed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a817",
   "metadata": {},
   "source": [
    "### Erste Datenexploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281400",
   "metadata": {},
   "source": [
    "Lassen Sie sich dann die ersten zehn Zeilen des RDDs anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766b363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:49.061163Z",
     "start_time": "2022-05-18T12:16:48.403315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "first_ten_lines = rdd_df.take(2)\n",
    "for line in first_ten_lines:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d99e5f",
   "metadata": {},
   "source": [
    "Die erste Zeile entspricht dem Header. Entfernen Sie diese aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcb216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:55.070359Z",
     "start_time": "2022-05-18T12:16:54.946567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "header = rdd_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65241bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_no_header = rdd_df.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f30438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:23:31.207384Z",
     "start_time": "2022-05-16T12:23:31.090921Z"
    }
   },
   "source": [
    "Zählen Sie nun die Anzahl der Angestellten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ff2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:17:03.912327Z",
     "start_time": "2022-05-18T12:17:03.303209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "num_employees = rdd_no_header.count()\n",
    "print(\"Anzahl der Angestellten: \", num_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c66d9c",
   "metadata": {},
   "source": [
    "### Datenverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc40b9",
   "metadata": {},
   "source": [
    "Zählen Sie nun die Angestellten pro Department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499b286",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Sollten Sie keinen Lösungsansatz haben, finden Sie unter dem Button \"Show Solution\" die Beschreibung der notwendigen Verarbeitungs-Schritte.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f383a4",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. Aktuell ist jede Zeile des RDD ein String, in dem die einzelnen Felder durch Komma getrennt sind. Dieser String muss zunächst gesplittet werden.\n",
    "2. Im nächsten Schritt müssen Key-Value-Paare gebildet werden. Der Key ist das Department und der Wert ist 1.\n",
    "3. Dann müssen pro Key alle Werte addiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71226012",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Wenn Sie Probleme mit der konkreten Umsetzung der Aufgabe haben, finden Sie unter dem Button \"Show Solution\" Hinweise zur Programmierung.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd24fd",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Das Splitten der einzelnen Funktionen erfolgt mit `split(\",\")`.\n",
    "- Anschließend liegen die Daten in einer Liste vor. Das dritte Element (`line[2]`) entspricht dem Department.\n",
    "- Mithilfe der Funktion `reducyByKey()` können die Werte pro Key addiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dda5795c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd_no_header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ihre Lösung\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Splitting\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m rdd_split \u001b[38;5;241m=\u001b[39m \u001b[43mrdd_no_header\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rdd_no_header' is not defined"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "#Splitting\n",
    "rdd_split = rdd_no_header.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key-Value-Paare erstellen:\n",
    "rdd_kv = rdd_split.map(lambda line: (line[2], 1))  # line[2] ist das Department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007612dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Werte pro Key addieren:\n",
    "rdd_counts = rdd_kv.reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for department, count in rdd_counts.collect():\n",
    "    print(department, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b975a2e",
   "metadata": {},
   "source": [
    "Die Arbeit mit RDDs ist offensichtlich recht mühsam, weshalb in der Praxis häufig eine High-level API wie SparkSQL verwendet wird. Diese schauen wir uns im Folgenden an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926af8d",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "Im Folgenden arbeiten wir mit der High-level API SparkSQL.\n",
    "\n",
    "*Vorsicht: DataFrames sind die zentrale Abstraktion der SparkSQL-Bibiliothek. Die Bibliothek bietet zwei praktisch gleichwertige APIs, um mit DataFrames zu arbeiten: Die **DataFrame API** und die **SQL API**. Wir arbeiten zunächst mit der DataFrame API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b8b35",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie die Daten aus der Datei `employees_satisfaction_transformed.csv` in ein DataFrame. Die Spaltenbezeichnungen sollen dabei im Header stehen und nicht Teil der Daten sein, nutzen Sie außerdem die automatische Schemaerkennung von Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f49c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:05.041562Z",
     "start_time": "2022-05-20T13:30:58.623713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af8746",
   "metadata": {},
   "source": [
    "Lassen Sie sich das Schema ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1cc9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:05.871051Z",
     "start_time": "2022-05-20T13:31:05.855291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82feb5",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Daten anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf2891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:07.726961Z",
     "start_time": "2022-05-20T13:31:07.392422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab027004",
   "metadata": {},
   "source": [
    "Geben Sie die Anzahl Angestellter aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c639af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:09.506780Z",
     "start_time": "2022-05-20T13:31:08.979554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc66789",
   "metadata": {},
   "source": [
    "DataFrames basieren auf RDDs. Deshalb kann auch jedes DataFrame in ein RDD umgewandelt werden.\n",
    "\n",
    "Erstellen Sie aus dem DataFrame der Angestellten ein RDD und lassen Sie sich die ersten fünf Zeilen anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74083f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:10.911287Z",
     "start_time": "2022-05-20T13:31:10.267345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b998b3e",
   "metadata": {},
   "source": [
    "Das RDD besteht aus einzelnen Rows. Mittels Index kann auf die einzelnen Rows zugegriffen werden. Der Zugriff auf die Spalten erfolgt entweder auch durch die Angabe des Index oder durch den Spaltennamen.\n",
    "\n",
    "Lassen Sie sich das Gehalt der ersten Row ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0a8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:12.144784Z",
     "start_time": "2022-05-20T13:31:11.878447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5c185",
   "metadata": {},
   "source": [
    "### Verarbeitung\n",
    "*Hinweis: Für die Bearbeitung der folgenden Aufaben ist es hilfreich, sich die `pyspark.sql.functions` anzusehen. Diese sind bereits importiert und können mit `f.<function_name>` verwendet werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0134cb",
   "metadata": {},
   "source": [
    "Fügen Sie eine neue Spalte `service_days` hinzu, in der angegeben wird, seit wie vielen Tagen ein\\*e Angestellte\\*r im Unternehmen ist. Verwenden Sie als Stichtag den 01.01.2021 (`comparison`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:13.934683Z",
     "start_time": "2022-05-20T13:31:13.931821Z"
    }
   },
   "outputs": [],
   "source": [
    "comparison = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b6240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:14.474401Z",
     "start_time": "2022-05-20T13:31:14.416892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc920c",
   "metadata": {},
   "source": [
    "Lassen Sie sich anschließend das Eintrittsdatum und die Tage im Unternehmen von allen Angestellten anzeigen, die länger als 6000 Tage im Unternehmen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2203dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:16.272880Z",
     "start_time": "2022-05-20T13:31:16.006660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0959",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971c46d",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Departments des Datensatzes anzeigen (jedes nur einmal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:23.044052Z",
     "start_time": "2022-05-20T13:31:22.674850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9ad56",
   "metadata": {},
   "source": [
    "Ermitteln Sie (wie eben bei der Arbeit mit RDDs) die Anzahl Angestellte pro Department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869911f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:24.046796Z",
     "start_time": "2022-05-20T13:31:23.715420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c8e3f",
   "metadata": {},
   "source": [
    "Geben Sie durchschnittliche Zufriedenheit aller Personen aus, die in der Abteilung *HR* arbeiten, sowie das Alter der ältesten Person der Abteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:26.924816Z",
     "start_time": "2022-05-20T13:31:26.677928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f5e3f",
   "metadata": {},
   "source": [
    "Wie erstellen nun ein weiteres DataFrame. Erstellen Sie aus den Daten in `dept_data` das DataFrame `departments` mit den angegebenen Spaltennamen. Geben Sie das erzeugte DataFrame aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884ec5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:30.025682Z",
     "start_time": "2022-05-20T13:31:30.018246Z"
    }
   },
   "outputs": [],
   "source": [
    "dept_data = [(\"Sales\", \"New York\", 10001), \n",
    "            (\"HR\", \"Los Angeles\", 90001),\n",
    "            (\"Purchasing\", \"San Francisco\", 94104),\n",
    "            (\"Marketing\", \"Philadelphia\", 19110),\n",
    "            (\"Technology\", \"Los Angeles\", 90099)]\n",
    "\n",
    "column_names = [\"name\", \"location\", \"zip_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:31.582707Z",
     "start_time": "2022-05-20T13:31:30.647816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbddabb",
   "metadata": {},
   "source": [
    "Wie viele Angestellte arbeiten in Los Angeles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91683a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:32.932839Z",
     "start_time": "2022-05-20T13:31:32.346598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794ada0",
   "metadata": {},
   "source": [
    "Lassen Sie sich die ersten 10 Gehälter der Angestellten ausgeben, die an der Ostküste arbeiten (in diesem Fall `zip_code < 90000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bffef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:35.699842Z",
     "start_time": "2022-05-20T13:31:35.467405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11623a09",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Durch die In-Memory-Verarbeitung werden DataFrames in der Regel nicht zwischengespeichert. Mit jeder Action werden alle Verarbeitungsschritte neu ausgeführt. Manchmal kann es aber sinnvoll sein, ein DataFrame zwischenzuspeichern (zu *cachen*). Beim Aufruf der ersten Action wird das DataFrame dann im Speicher gehalten.\n",
    "\n",
    "(*Hinweis*: Gleiches gilt natürlich auch für RDDs, da DataFrames ja auf RDDs basieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "Betrachten Sie den folgenden Code: An welcher Stelle könnte es sinnvoll sein, ein Caching durchzuführen und warum? Fügen Sie den Code für das Caching ein. Schreiben Sie in die untere Zelle Ihre Begründung und beschreiben Sie, welche DataFrames auf die gecachte Version zugreifen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e30d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "```python\n",
    "df1 = spark.read.csv(path = \"my_data.csv\", header = True)\n",
    "df2 = df1.filter(df1.salary.between(40000, 50000))\n",
    "df3 = df2.select(\"name\", \"salary\", \"department\", \"satisfied\")\n",
    "df4 = df3.groupby(\"department\").agg({\"satisfied\": \"mean\"})\n",
    "df4.show()\n",
    "df5 = df3.groupby(\"department\").agg({\"salary\": \"mean\"})\n",
    "df5.show()\n",
    "df6 = df3.select(\"name\").where(df3[\"satisfied\"] == 1)\n",
    "df6.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "878fb55f",
   "metadata": {},
   "source": [
    "Ihre Begründung:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558d965",
   "metadata": {},
   "source": [
    "## SQL\n",
    "Im Folgenden verwenden wir SQL zur Datenabfrage. Dafür erstellen wir zunächst eine temporale View, auf der dann die SQL-Abfragen erfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d524019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:52.806431Z",
     "start_time": "2022-05-18T12:26:51.663985Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\\\n",
    "CREATE TEMP VIEW employees \\\n",
    "USING CSV \\\n",
    "OPTIONS (path = 'employees_satisfaction_transformed.csv', header = 'True', inferSchema = 'True')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3219ad",
   "metadata": {},
   "source": [
    "Ermitteln Sie nun wieder die Anzahl der Angestellten pro Department, aber verwenden Sie dieses Mal SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30abf3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:54.656380Z",
     "start_time": "2022-05-18T12:26:54.394996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb4653",
   "metadata": {},
   "source": [
    "## Ressourcen-Vebrauch\n",
    "\n",
    "Sie arbeiten in diesen Übungsaufgaben mit ihrem eigenen Cluster. In der Praxis ist dies natürlich selten der Fall, hier werden meist viele Jobs gleichzeitig gestartet und konkurrieren um Ressourcen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ba7dd",
   "metadata": {},
   "source": [
    "Öffnen Sie die __[WebUI von YARN](http://127.0.0.1:8088)__ und sehen sich die aktuellen Applikationen an. Sie sollten dort das aktuelle Jupyter Notebook finden. Solange die SparkSession (der SparkContext) erstellt ist, wird das Notebook dort als *RUNNING* aufgelistet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbe4f3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Wie viele Kerne und wie viel Speicher verbraucht die Spark-Applikation? Wie können Sie diese Summe berechnen?\n",
    "\n",
    "*Hinweis: Werfen Sie einen Blick in die Datei `spark-defaults.conf` im `SPARK_CONF_DIR`.*\n",
    "\n",
    "*Sollten Sie nicht wissen, wie Sie diese Datei finden können, finden Sie unter dem \"Show Solution\"-Button hilfreiche Kommandos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a9af2",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Über \"New\" -> \"Terminal\" können Sie ein Terminal öffnen. Alternativ können Sie in einer Code-Zelle einen Bash-Befehl ausführen, indem Sie ein ! an den Anfang der Zelle schreiben\n",
    "- Zugriff auf den Inhalt einer Umgebungs-Variablen mittels `$`, z.B. `$SPARK_CONF_DIR`\n",
    "- Anzeigen von Dateiinhalten: `cat`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87b95f3",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953cbef",
   "metadata": {},
   "source": [
    "Stoppen Sie den SparkContext, um alle Ressourcen, die dieses Notebook benötigt, freizugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50743887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-17T07:34:36.480924Z",
     "start_time": "2022-08-17T07:34:36.474478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba990b91",
   "metadata": {},
   "source": [
    "Öffnen Sie ein Terminal und starten Sie eine pyspark-Shell mit YARN als Ressourcen-Manager im client-mode mit 2 Exekutoren mit jeweils 1GB Memory und 2 Cores. Wie viele Cores und wie viel Speicher benötigt die Applikation und wie setzen sich diese Zahlen zusammen? Kontrollen Sie in der WebUI von YARN, ob Ihre Berechnungen korrekt sind."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a8b432",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980a463",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal eine weitere pyspark-Shell (ohne die erste Shell zu beenden) mit YARN als Ressourcen-Manager im client-mode mit der gleichen Konfiguration (2 Exekutoren mit jeweils 1GB Memory und 2 Cores). Erstellen Sie dort ein RDD mit den Werten 1, 2, 3, 4 und lassen sich dieses mittels `collect()` ausgeben.\n",
    "\n",
    "Was passiert und warum? Wie können Sie das Problem beheben?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc390ab8",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b01a8",
   "metadata": {},
   "source": [
    "Schließen Sie beide Shells und starten anschließend eine neue pyspark-Shell mit folgenden Eigenschaften:\n",
    "- YARN als Ressourcen-Manager im client-mode\n",
    "- 2 Exekutoren mit jeweils\n",
    "    - 2GB Memory\n",
    "    - 3 Cores\n",
    "    \n",
    "Wie viele Ressourcen vebraucht diese Applikation und wie setzen sich diese zusammen?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d7b3eac",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fc5a1",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal noch eine pypsark-Shell (ohne die erste Shell zu beenden) mit folgendem Befehl:\n",
    "\n",
    "`pyspark --master yarn --conf spark.yarn.am.cores=3`\n",
    "\n",
    "Was passiert und warum? Überprüfen Sie auch den Status der Applikation in der WebUI von YARN."
   ]
  },
  {
   "cell_type": "raw",
   "id": "abaeaf3c",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
